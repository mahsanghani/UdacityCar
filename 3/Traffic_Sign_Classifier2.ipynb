{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import scipy.ndimage\n",
    "import cv2\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import sys\n",
    "# import tensorflow_addons as tfa\n",
    "# from tfa.layers import flatten\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_file = \"data/train.pickle\"\n",
    "validation_file= \"data/valid.pickle\"\n",
    "testing_file = \"data/test.pickle\"\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_valid, y_valid = valid['features'], valid['labels']\n",
    "X_test, y_test = test['features'], test['labels']\n",
    "\n",
    "assert(len(X_train) == len(y_train))\n",
    "assert(len(X_valid) == len(y_valid))\n",
    "assert(len(X_test) == len(y_test))\n",
    "\n",
    "print(\"X_train shape:\", X_train[0].shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_valid shape:\", X_valid[0].shape)\n",
    "print(\"y_valid shape:\", y_valid.shape)\n",
    "print(\"X_test shape:\", X_test[0].shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_train = len(X_train)\n",
    "n_valid = len(X_valid)\n",
    "n_test = len(X_test)\n",
    "image_shape = X_train[0].shape\n",
    "n_classes = len(np.unique(y_train))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Number of training examples = \", n_train)\n",
    "print(\"Number of validation example = \", n_valid)\n",
    "print(\"Number of testing samples = \", n_test)\n",
    "print(\"Shape of first training image = \", image_shape[0], \"w x\", image_shape[1], \"h x\", image_shape[2], \"d\")\n",
    "print(\"Number of classes = \", n_classes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Visualizations will be shown in the notebook.\n",
    "%matplotlib inline\n",
    "\n",
    "rand_train_indices = np.random.randint(0, X_train.shape[0], 10)\n",
    "rand_valid_indices = np.random.randint(0, X_valid.shape[0], 10)\n",
    "rand_test_indices = np.random.randint(0, X_test.shape[0], 10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 5))\n",
    "for index in range(len(rand_train_indices)):\n",
    "    sub_plt = plt.subplot(2, 5, index + 1)\n",
    "    sub_plt.imshow(X_train[rand_train_indices[index]])\n",
    "    sub_plt.text(5, 5, y_train[rand_train_indices[index]], bbox=dict(facecolor='r', alpha=0.5))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 5))\n",
    "for index in range(len(rand_valid_indices)):\n",
    "    sub_plt = plt.subplot(2, 5, index + 1)\n",
    "    sub_plt.imshow(X_valid[rand_valid_indices[index]])\n",
    "    sub_plt.text(5, 5, y_valid[rand_valid_indices[index]], bbox=dict(facecolor='y', alpha=0.5))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 5))\n",
    "for index in range(len(rand_test_indices)):\n",
    "    sub_plt = plt.subplot(2, 5, index + 1)\n",
    "    sub_plt.imshow(X_test[rand_test_indices[index]])\n",
    "    sub_plt.text(5, 5, y_test[rand_test_indices[index]], bbox=dict(facecolor='g', alpha=0.5))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "sub_plt = plt.subplot(3, 1, 1)\n",
    "train_hist = sub_plt.hist(y_train, bins=range(y_train.max() + 1))\n",
    "\n",
    "sub_plt = plt.subplot(3, 1, 2)\n",
    "valid_hist = sub_plt.hist(y_valid, bins=range(y_valid.max() + 1))\n",
    "\n",
    "sub_plt = plt.subplot(3, 1, 3)\n",
    "test_hist = sub_plt.hist(y_test, bins=range(y_test.max() + 1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rand_train_ind = np.arange(X_train.shape[0])\n",
    "rand_valid_ind = np.arange(X_valid.shape[0])\n",
    "rand_test_ind = np.arange(X_test.shape[0])\n",
    "\n",
    "np.random.shuffle(rand_train_ind)\n",
    "np.random.shuffle(rand_valid_ind)\n",
    "np.random.shuffle(rand_test_ind)\n",
    "\n",
    "X_train = X_train[rand_train_ind].astype(np.float32)\n",
    "X_valid = X_valid[rand_valid_ind].astype(np.float32)\n",
    "X_test = X_test[rand_test_ind].astype(np.float32)\n",
    "\n",
    "y_train = y_train[rand_train_ind]\n",
    "y_valid = y_valid[rand_valid_ind]\n",
    "y_test = y_test[rand_test_ind]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train = (X_train - 128.0) / 128.0\n",
    "X_valid = (X_valid - 128.0) / 128.0\n",
    "X_test = (X_test - 128.0) / 128.0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_oh_train = np.zeros((y_train.shape[0], y_train.max() + 1), dtype=np.float32)\n",
    "y_oh_train[np.arange(y_train.shape[0], dtype=np.uint32), y_train] = 1.0\n",
    "\n",
    "y_oh_valid = np.zeros((y_valid.shape[0], y_valid.max() + 1), dtype=np.float32)\n",
    "y_oh_valid[np.arange(y_valid.shape[0], dtype=np.uint32), y_valid] = 1.0\n",
    "\n",
    "y_oh_test = np.zeros((y_test.shape[0], y_test.max() + 1), dtype=np.float32)\n",
    "y_oh_test[np.arange(y_test.shape[0], dtype=np.uint32), y_test] = 1.0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import sys"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Cnn5(object):\n",
    "\n",
    "    def __init__(self, features, filters, dense_size, num_out, nn_scope='CNN_Net'):\n",
    "\n",
    "        self._filters = filters\n",
    "        self._dense_size = dense_size\n",
    "        self._num_out = num_out\n",
    "        self._nn_scope = nn_scope\n",
    "\n",
    "        with tf.variable_scope(self._nn_scope):\n",
    "\n",
    "            self.conv_1 = tf.layers.conv2d(features, filters=self._filters[0], kernel_size=(3, 3),\n",
    "                                            strides=(1, 1), padding=\"VALID\",\n",
    "                                            kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                            bias_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                            activation=tf.nn.leaky_relu, name=\"Conv_1\")\n",
    "\n",
    "            self.conv_2 = tf.layers.conv2d(self.conv_1, filters=self._filters[1], kernel_size=(3, 3),\n",
    "                                            strides=(1, 1), padding=\"VALID\",\n",
    "                                            kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                            bias_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                            activation=tf.nn.leaky_relu, name=\"Conv_2\")\n",
    "\n",
    "            self.maxpool_1 = tf.layers.max_pooling2d(self.conv_2, pool_size=(2, 2), strides=(2, 2), name=\"MPool_1\")\n",
    "\n",
    "            self.conv_3 = tf.layers.conv2d(self.maxpool_1, filters=self._filters[2], kernel_size=(3, 3),\n",
    "                                            strides=(1, 1), padding=\"VALID\",\n",
    "                                            kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                            bias_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                            activation=tf.nn.leaky_relu, name=\"Conv_3\")\n",
    "\n",
    "            self.conv_4 = tf.layers.conv2d(self.conv_3, filters=self._filters[3], kernel_size=(3, 3),\n",
    "                                            strides=(1, 1), padding=\"VALID\",\n",
    "                                            kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                            bias_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                            activation=tf.nn.leaky_relu, name=\"Conv_4\")\n",
    "\n",
    "            self.maxpool_2 = tf.layers.max_pooling2d(self.conv_4, pool_size=(2, 2), strides=(2, 2), name=\"MPool_2\")\n",
    "\n",
    "\n",
    "            self.conv_5 = tf.layers.conv2d(self.maxpool_2, filters=self._filters[4], kernel_size=(3, 3),\n",
    "                                            strides=(1, 1), padding=\"VALID\",\n",
    "                                            kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                            bias_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                            activation=tf.nn.leaky_relu, name=\"Conv_5\")\n",
    "\n",
    "            self.flat = tf.layers.flatten(inputs=self.conv_5, name=\"Flat_Conv_5\")\n",
    "\n",
    "            self.fc_1 = tf.layers.dense(self.flat, units=self._dense_size[0],\n",
    "                                         kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                         bias_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                         activation=tf.nn.leaky_relu, name=\"FC_1\")\n",
    "\n",
    "            self.fc_2 = tf.layers.dense(self.fc_1, units=self._dense_size[1],\n",
    "                                         kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                         bias_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                         activation=tf.nn.leaky_relu, name=\"FC_2\")\n",
    "\n",
    "            self.logits = tf.layers.dense(self.fc_2, units=self._num_out,\n",
    "                                           kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                           bias_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                           name=\"Logits\")\n",
    "\n",
    "            self.prediction = tf.nn.softmax(self.logits)\n",
    "\n",
    "            self.output = tf.argmax(self.logits, 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "filters = [16, 24, 48, 64, 128]\n",
    "dense_size = [384, 256]\n",
    "\n",
    "with tf.variable_scope('Placeholders'):\n",
    "    # Lets define the requied placeholders\n",
    "    features = tf.placeholder(tf.float32, shape=[None] + list(image_shape), name=\"Features\")\n",
    "    labels = tf.placeholder(dtype=tf.float32, shape=[None] + list(y_oh_train.shape[1:]), name=\"Labels\")\n",
    "    learning_rate = tf.placeholder(dtype=tf.float32, name=\"Learning_rate\")\n",
    "\n",
    "\n",
    "# Using these placeholders, lets construct our Neural Network\n",
    "cnn5 = Cnn5(features, filters, dense_size, labels.shape[1])\n",
    "# Now lets define the cost\n",
    "with tf.variable_scope('Cost'):\n",
    "    entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=cnn5.logits, labels=labels, name='loss')\n",
    "    cost_t = tf.reduce_mean(entropy)\n",
    "# Accuracy may help us evaluate our training better:\n",
    "with tf.variable_scope('Accuracy'):\n",
    "    correct_preds = tf.equal(cnn5.output, tf.argmax(labels, 1))\n",
    "    accuracy_t = tf.reduce_mean(tf.cast(correct_preds, tf.float32))\n",
    "# Finally define the optimizer:\n",
    "with tf.variable_scope('Optimizer'):\n",
    "    optimize_t = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost_t)\n",
    "writer_train = tf.summary.FileWriter(\"./tensorboard/\", graph=tf.get_default_graph())\n",
    "writer_train.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "Train Epoch 4:   0%|          | 0/34799 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f4572b12410f45d2a9ce7eb44f567da7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Train Epoch 5:   0%|          | 0/34799 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "55f0a794cb4b4a5ca79677c04046f92c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Train Epoch 6:   0%|          | 0/34799 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1a8392f7077849b69176fd36c2ca53c1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Train Epoch 7:   0%|          | 0/34799 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "163b4caceaf5425d9b7125619950e7c0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Train Epoch 8:   0%|          | 0/34799 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "99bbf71667044a29b2705e06cc13a7f0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Train Epoch 9:   0%|          | 0/34799 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "559c665ee3764c978e55ea4f06312483"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Train Epoch 10:   0%|          | 0/34799 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c77d877d365d4bedb4159a2a43608084"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Epoch Cost: 0.012954, Final Epoch Accuracy: 0.995885\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "# Definition of One Epoch of training\n",
    "def train_epoch(epoch, rate):\n",
    "\n",
    "    prog_bar = tqdm(desc=\"Train Epoch %d\" % epoch, total=n_train)\n",
    "\n",
    "    error_list, accuracy_list = [], []\n",
    "    low, high, step_size, end = 0, n_train + 1, BATCH_SIZE, 0\n",
    "\n",
    "    for start, end in zip(range(low, high, step_size), range(low + step_size, high, step_size)):\n",
    "        _, error, accuracy = sess.run([optimize_t, cost_t, accuracy_t],\n",
    "            feed_dict={features: X_train[start:end],labels: y_oh_train[start:end],learning_rate: rate})\n",
    "\n",
    "        prog_bar.set_postfix_str(\"Cost: %f, Acc: %f\" % (error, accuracy), refresh=True)\n",
    "        prog_bar.update(n=BATCH_SIZE)\n",
    "        error_list.append(error)\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "    error = sum(error_list) / len(error_list)\n",
    "    accuracy = sum(accuracy_list) / len(accuracy_list)\n",
    "    prog_bar.set_postfix_str(\"Cost: %f, Acc: %f\" % (error, accuracy), refresh=True)\n",
    "    prog_bar.close()\n",
    "    sys.stdout.flush()\n",
    "    return error, accuracy\n",
    "sess_config = tf.ConfigProto(device_count={'GPU': 1})\n",
    "sess_config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=sess_config)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Start the training\n",
    "for index in range(NUM_EPOCHS // 2):\n",
    "    train_epoch(index + 1, LEARNING_RATE)\n",
    "\n",
    "for index in range(NUM_EPOCHS // 2, NUM_EPOCHS):\n",
    "    error, accuracy = train_epoch(index + 1, LEARNING_RATE / 2)\n",
    "\n",
    "saver.save(sess, './saved_models/lane')\n",
    "sess.close()\n",
    "\n",
    "print(\"Final Epoch Cost: %f, Final Epoch Accuracy: %f\" %(error, accuracy))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./saved_models/lane\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ghani\\AppData\\Local\\Temp\\ipykernel_4464\\824994229.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  prog_bar = tqdm(desc=\"Valid Epoch %d\" % epoch, total=n_valid)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Valid Epoch 1:   0%|          | 0/4410 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "79df0a4552074b3180952cfa78caae3b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Epoch Cost: 0.325447, Validation Epoch Accuracy: 0.944393\n"
     ]
    }
   ],
   "source": [
    "def valid_epoch(epoch):\n",
    "\n",
    "    prog_bar = tqdm(desc=\"Valid Epoch %d\" % epoch, total=n_valid)\n",
    "\n",
    "    error_list, accuracy_list = [], []\n",
    "    low, high, step_size, end = 0, n_valid + 1, BATCH_SIZE, 0\n",
    "\n",
    "    for start, end in zip(range(low, high, step_size), range(low + step_size, high, step_size)):\n",
    "        error, accuracy = sess.run([cost_t, accuracy_t],\n",
    "            feed_dict={features: X_valid[start:end],labels: y_oh_valid[start:end]})\n",
    "\n",
    "        prog_bar.set_postfix_str(\"Cost: %f, Acc: %f\" % (error, accuracy), refresh=True)\n",
    "        prog_bar.update(n=BATCH_SIZE)\n",
    "        error_list.append(error)\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "    error = sum(error_list) / len(error_list)\n",
    "    accuracy = sum(accuracy_list) / len(accuracy_list)\n",
    "    prog_bar.set_postfix_str(\"Cost: %f, Acc: %f\" % (error, accuracy), refresh=True)\n",
    "    prog_bar.close()\n",
    "    sys.stdout.flush()\n",
    "    return error, accuracy\n",
    "# Start the validation\n",
    "sess = tf.Session(config=sess_config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver.restore(sess, tf.train.latest_checkpoint('./saved_models/'))\n",
    "error, accuracy = valid_epoch(1)\n",
    "sess.close()\n",
    "print(\"Validation Epoch Cost: %f, Validation Epoch Accuracy: %f\" %(error, accuracy))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./saved_models/lane\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ghani\\AppData\\Local\\Temp\\ipykernel_4464\\3892152692.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  prog_bar = tqdm(desc=\"Test Epoch %d\" % epoch, total=n_test)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Test Epoch 1:   0%|          | 0/12630 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bfaf931e711f4365ad15a81852822802"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch Cost: 0.452889, Test Epoch Accuracy: 0.935041\n"
     ]
    }
   ],
   "source": [
    "def test_epoch(epoch):\n",
    "\n",
    "    prog_bar = tqdm(desc=\"Test Epoch %d\" % epoch, total=n_test)\n",
    "\n",
    "    error_list, accuracy_list = [], []\n",
    "    low, high, step_size, end = 0, n_test + 1, BATCH_SIZE, 0\n",
    "\n",
    "    for start, end in zip(range(low, high, step_size), range(low + step_size, high, step_size)):\n",
    "        error, accuracy = sess.run([cost_t, accuracy_t],\n",
    "            feed_dict={features: X_test[start:end],labels: y_oh_test[start:end]})\n",
    "\n",
    "        prog_bar.set_postfix_str(\"Cost: %f, Acc: %f\" % (error, accuracy), refresh=True)\n",
    "        prog_bar.update(n=BATCH_SIZE)\n",
    "        error_list.append(error)\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "    error = sum(error_list) / len(error_list)\n",
    "    accuracy = sum(accuracy_list) / len(accuracy_list)\n",
    "    prog_bar.set_postfix_str(\"Cost: %f, Acc: %f\" % (error, accuracy), refresh=True)\n",
    "    prog_bar.close()\n",
    "    sys.stdout.flush()\n",
    "    return error, accuracy\n",
    "# Start the testing\n",
    "sess = tf.Session(config=sess_config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver.restore(sess, tf.train.latest_checkpoint('./saved_models/'))\n",
    "error, accuracy = test_epoch(1)\n",
    "sess.close()\n",
    "print(\"Test Epoch Cost: %f, Test Epoch Accuracy: %f\" %(error, accuracy))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1600x500 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "from glob import glob\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "# Now lets read the image paths from the file system. These images are part of the test dataset of GTSRB.\n",
    "image_paths = glob('./gtsrb_test_data/*.png')\n",
    "\n",
    "images = []\n",
    "for img_path in image_paths:\n",
    "    img = mpimg.imread(img_path)\n",
    "    resized_img = cv2.resize(img, (32, 32))\n",
    "    images.append(resized_img)\n",
    "\n",
    "# Lets Visualize some random samples from the test dataset\n",
    "plt.figure(figsize=(16, 5))\n",
    "for index in range(len(images)):\n",
    "    sub_plt = plt.subplot(2, 5, index + 1)\n",
    "    sub_plt.imshow(images[index])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./saved_models/lane\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (0,) for Tensor Placeholders/Features:0, which has shape (?, 32, 32, 3)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 8\u001B[0m\n\u001B[0;32m      5\u001B[0m sess\u001B[38;5;241m.\u001B[39mrun(tf\u001B[38;5;241m.\u001B[39mglobal_variables_initializer())\n\u001B[0;32m      6\u001B[0m saver\u001B[38;5;241m.\u001B[39mrestore(sess, tf\u001B[38;5;241m.\u001B[39mtrain\u001B[38;5;241m.\u001B[39mlatest_checkpoint(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./saved_models/\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[1;32m----> 8\u001B[0m [output] \u001B[38;5;241m=\u001B[39m \u001B[43msess\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mcnn5\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeed_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mimages\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m sess\u001B[38;5;241m.\u001B[39mclose()\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m(output)\n",
      "File \u001B[1;32m~\\PycharmProjects\\UdacityCar\\venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py:968\u001B[0m, in \u001B[0;36mBaseSession.run\u001B[1;34m(self, fetches, feed_dict, options, run_metadata)\u001B[0m\n\u001B[0;32m    965\u001B[0m run_metadata_ptr \u001B[38;5;241m=\u001B[39m tf_session\u001B[38;5;241m.\u001B[39mTF_NewBuffer() \u001B[38;5;28;01mif\u001B[39;00m run_metadata \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    967\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 968\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfetches\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeed_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions_ptr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    969\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mrun_metadata_ptr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    970\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m run_metadata:\n\u001B[0;32m    971\u001B[0m     proto_data \u001B[38;5;241m=\u001B[39m tf_session\u001B[38;5;241m.\u001B[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001B[1;32m~\\PycharmProjects\\UdacityCar\\venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1165\u001B[0m, in \u001B[0;36mBaseSession._run\u001B[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001B[0m\n\u001B[0;32m   1161\u001B[0m   np_val \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(subfeed_val, dtype\u001B[38;5;241m=\u001B[39msubfeed_dtype)\n\u001B[0;32m   1163\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;129;01mnot\u001B[39;00m is_tensor_handle_feed \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[0;32m   1164\u001B[0m     \u001B[38;5;129;01mnot\u001B[39;00m subfeed_t\u001B[38;5;241m.\u001B[39mget_shape()\u001B[38;5;241m.\u001B[39mis_compatible_with(np_val\u001B[38;5;241m.\u001B[39mshape)):\n\u001B[1;32m-> 1165\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1166\u001B[0m       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCannot feed value of shape \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(np_val\u001B[38;5;241m.\u001B[39mshape)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for Tensor \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m   1167\u001B[0m       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msubfeed_t\u001B[38;5;241m.\u001B[39mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, which has shape \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m   1168\u001B[0m       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(subfeed_t\u001B[38;5;241m.\u001B[39mget_shape())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m   1169\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgraph\u001B[38;5;241m.\u001B[39mis_feedable(subfeed_t):\n\u001B[0;32m   1170\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTensor \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msubfeed_t\u001B[38;5;241m.\u001B[39mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m may not be fed.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mValueError\u001B[0m: Cannot feed value of shape (0,) for Tensor Placeholders/Features:0, which has shape (?, 32, 32, 3)"
     ]
    }
   ],
   "source": [
    "true_labels = np.array([2, 18, 1, 14, 12, 5, 3, 36, 33, 4])\n",
    "\n",
    "# Start the testing\n",
    "sess = tf.Session(config=sess_config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver.restore(sess, tf.train.latest_checkpoint('./saved_models/'))\n",
    "\n",
    "[output] = sess.run([cnn5.output], feed_dict={features: images})\n",
    "sess.close()\n",
    "\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "accuracy = np.mean(np.equal(true_labels, output)) * 100\n",
    "print(\"Performance is %f \" % accuracy)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sess = tf.Session(config=sess_config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver.restore(sess, tf.train.latest_checkpoint('./saved_models/'))\n",
    "\n",
    "top_5_prob = tf.nn.top_k(cnn5.prediction, k=5)\n",
    "\n",
    "top_prob = sess.run(top_5_prob, feed_dict={features: images})\n",
    "sess.close()\n",
    "\n",
    "print(top_prob)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def outputFeatureMap(image_input, tf_activation, activation_min=-1, activation_max=-1 ,plt_num=1):\n",
    "    # Here make sure to preprocess your image_input in a way your network expects\n",
    "    # with size, normalization, ect if needed\n",
    "    # image_input =\n",
    "    # Note: x should be the same name as your network's tensorflow data placeholder variable\n",
    "    # If you get an error tf_activation is not defined it may be having trouble accessing the variable from inside a function\n",
    "    activation = tf_activation.eval(session=sess,feed_dict={x : image_input})\n",
    "    featuremaps = activation.shape[3]\n",
    "    plt.figure(plt_num, figsize=(15,15))\n",
    "    for featuremap in range(featuremaps):\n",
    "        plt.subplot(6,8, featuremap+1) # sets the number of feature maps to show on each row and column\n",
    "        plt.title('FeatureMap ' + str(featuremap)) # displays the feature map number\n",
    "        if activation_min != -1 & activation_max != -1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmin =activation_min, vmax=activation_max, cmap=\"gray\")\n",
    "        elif activation_max != -1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmax=activation_max, cmap=\"gray\")\n",
    "        elif activation_min !=-1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmin=activation_min, cmap=\"gray\")\n",
    "        else:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", cmap=\"gray\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
